#!/usr/bin/env python3
"""CLI script to run lens injection experiments.

Usage:
    # Quick comparison (2 tasks, 1 run each)
    python scripts/run_lens_experiments.py --quick

    # Full evaluation (all tasks, 3 runs each)
    python scripts/run_lens_experiments.py --full

    # Single strategy test
    python scripts/run_lens_experiments.py --strategy xml_structured

    # Specific tasks
    python scripts/run_lens_experiments.py --tasks divide,sort,fibonacci

    # With specific model
    python scripts/run_lens_experiments.py --model claude-3-sonnet --provider anthropic
"""

import argparse
import asyncio
import json
import sys
from datetime import datetime
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


async def main() -> int:
    parser = argparse.ArgumentParser(
        description="Run lens injection experiments",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )

    parser.add_argument(
        "--quick",
        action="store_true",
        help="Quick comparison: 2 tasks, 1 run each (default)",
    )
    parser.add_argument(
        "--full",
        action="store_true",
        help="Full evaluation: all tasks, 3 runs each",
    )
    parser.add_argument(
        "--strategy",
        type=str,
        help="Run only a specific strategy (e.g., xml_structured, hybrid_optimized)",
    )
    parser.add_argument(
        "--tasks",
        type=str,
        help="Comma-separated list of tasks (e.g., divide,sort,fibonacci)",
    )
    parser.add_argument(
        "--runs",
        type=int,
        default=1,
        help="Runs per task/strategy combination (default: 1)",
    )
    parser.add_argument(
        "--model",
        type=str,
        help="Model name (default: from config)",
    )
    parser.add_argument(
        "--provider",
        type=str,
        help="Model provider (default: from config)",
    )
    parser.add_argument(
        "--output",
        type=str,
        help="Save results to JSON file",
    )
    parser.add_argument(
        "--show-prompts",
        action="store_true",
        help="Show the prompts generated by each strategy",
    )

    args = parser.parse_args()

    # Import after parsing to avoid slow imports on --help
    from sunwell.interface.cli.helpers import resolve_model
    from sunwell.foundation.config import get_config
    from sunwell.demo.experiment_runner import (
        ExperimentConfig,
        ExperimentRunner,
        print_detailed_report,
        print_experiment_report,
    )
    from sunwell.demo.lens_experiments import (
        LensStrategy,
        create_prompt_builder,
        load_default_lens,
    )
    from sunwell.demo.tasks import BUILTIN_TASKS

    # Resolve model
    config = get_config()
    provider = args.provider or (config.model.default_provider if config else "ollama")
    model_name = args.model or (config.model.default_model if config else "gemma3:4b")

    print(f"Using model: {provider}/{model_name}")

    model = resolve_model(provider, model_name)
    if not model:
        print("ERROR: Could not resolve model")
        return 1

    # Show prompts mode
    if args.show_prompts:
        lens = load_default_lens()
        task_prompt = "Write a Python function to divide two numbers"

        print("\n" + "=" * 70)
        print("LENS INJECTION STRATEGY PROMPTS")
        print("=" * 70)

        for strategy in LensStrategy:
            builder = create_prompt_builder(strategy, lens)
            prompt = builder.build_prompt(task_prompt)

            print(f"\n{'='*70}")
            print(f"STRATEGY: {strategy.value}")
            print("=" * 70)
            print(prompt)
            print()

        return 0

    # Configure experiment
    strategies = list(LensStrategy)
    if args.strategy:
        try:
            strategies = [LensStrategy(args.strategy)]
        except ValueError:
            print(f"ERROR: Unknown strategy '{args.strategy}'")
            print(f"Available: {[s.value for s in LensStrategy]}")
            return 1

    tasks = list(BUILTIN_TASKS.keys())
    if args.tasks:
        tasks = args.tasks.split(",")
        for t in tasks:
            if t not in BUILTIN_TASKS:
                print(f"ERROR: Unknown task '{t}'")
                print(f"Available: {list(BUILTIN_TASKS.keys())}")
                return 1

    runs = args.runs
    if args.quick:
        tasks = ["divide", "sort"]
        runs = 1
    elif args.full:
        runs = 3

    exp_config = ExperimentConfig(
        strategies=strategies,
        tasks=tasks,
        runs_per_combination=runs,
    )

    total = len(strategies) * len(tasks) * runs
    print(f"\nRunning {total} experiments:")
    print(f"  Strategies: {len(strategies)}")
    print(f"  Tasks: {len(tasks)}")
    print(f"  Runs per combination: {runs}")
    print()

    # Run experiments
    runner = ExperimentRunner(model, config=exp_config)

    def on_progress(**kwargs):
        pct = kwargs["current"] / kwargs["total"] * 100
        print(
            f"  [{pct:5.1f}%] {kwargs['strategy'].value:<20} "
            f"task={kwargs['task']:<15} run={kwargs['run_idx']}"
        )

    try:
        summaries = await runner.run_all(on_progress=on_progress)
    except KeyboardInterrupt:
        print("\n\nInterrupted! Showing partial results...")
        summaries = runner._compute_summaries()

    # Print report
    print("\n")
    print(print_experiment_report(summaries))

    if args.full:
        print("\n")
        print(print_detailed_report(summaries))

    # Save results
    if args.output:
        output_data = {
            "timestamp": datetime.now().isoformat(),
            "model": f"{provider}/{model_name}",
            "config": {
                "strategies": [s.value for s in strategies],
                "tasks": tasks,
                "runs_per_combination": runs,
            },
            "summaries": {
                s.value: {
                    "avg_score": summary.avg_score,
                    "success_rate": summary.success_rate,
                    "total_runs": summary.total_runs,
                }
                for s, summary in summaries.items()
            },
            "results": [
                {
                    "strategy": r.strategy.value,
                    "task": r.task_name,
                    "score": r.score,
                    "features_achieved": list(r.features_achieved),
                    "features_missing": list(r.features_missing),
                    "time_ms": r.time_ms,
                    "prompt_tokens": r.prompt_tokens,
                    "completion_tokens": r.completion_tokens,
                    "code": r.code,  # Include actual generated code
                }
                for r in runner.results
            ],
        }

        Path(args.output).write_text(json.dumps(output_data, indent=2))
        print(f"\nResults saved to: {args.output}")

    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
